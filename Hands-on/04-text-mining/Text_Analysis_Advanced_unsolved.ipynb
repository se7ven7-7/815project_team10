{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/se7ven7-7/815project_team10/blob/main/Hands-on/04-text-mining/Text_Analysis_Advanced_unsolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Course: BA820 - Unsupervised and Unstructured ML**\n",
        "\n",
        "**Notebook created by: Mohannad Elhamod**"
      ],
      "metadata": {
        "id": "jtWlvRw2YUxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Intuition Behind Word2Vec"
      ],
      "metadata": {
        "id": "D0JjP9q0g2n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how Word2Vec works, we will create a toy model by training it on a  small number of sentences.\n",
        "\n",
        "This is not a common practice. Generally, we just use a *pre-trained* model that was fitted to millions of sentences. Such models will be of high quality.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SEvf4j8SneHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpVe_1HGrqLJ",
        "outputId": "4132dfed-143f-4382-8846-f72babd5f751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) # Get the set of stop words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SN95pjRhwva",
        "outputId": "c8c3f7fa-d091-4355-9c07-e697dd963e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is some code that could clean text up"
      ],
      "metadata": {
        "id": "-Ak4-zC1tloy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def cleanup_text(sentence):\n",
        "  # First, word tokenize.\n",
        "  tokenized_sms_messages = word_tokenize(sentence)\n",
        "\n",
        "  # Lower case\n",
        "  tokenized_sms_messages = [word.lower() for word in tokenized_sms_messages]\n",
        "\n",
        "  # Remove punctuation\n",
        "  tokenized_sms_messages = [word for word in tokenized_sms_messages if word not in string.punctuation]\n",
        "\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenized_sms_messages = [word for word in tokenized_sms_messages if word not in stop_words]\n",
        "\n",
        "  # Stem\n",
        "  tokenized_sms_messages = [ps.stem(word) for word in tokenized_sms_messages]\n",
        "\n",
        "  return tokenized_sms_messages"
      ],
      "metadata": {
        "id": "lURlJqfctlHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'I love sleeping in my bed',\n",
        "    'He hates eating at McDonalds every night',\n",
        "    'I love drinking root beer',\n",
        "    'He hates studying physics textbooks',\n",
        "    'I love traveling to Europe every summer',\n",
        "    'He hates swimming in the big pool',\n",
        "]\n",
        "\n",
        "# Tokenize first.\n",
        "tokenized_corpus = [cleanup_text(sentence) for sentence in corpus]\n",
        "tokenized_corpus"
      ],
      "metadata": {
        "id": "Z_O4-IhZr0Cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b45fc3-6cba-4a74-888e-309ebeb1c8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['love', 'sleep', 'bed'],\n",
              " ['hate', 'eat', 'mcdonald', 'everi', 'night'],\n",
              " ['love', 'drink', 'root', 'beer'],\n",
              " ['hate', 'studi', 'physic', 'textbook'],\n",
              " ['love', 'travel', 'europ', 'everi', 'summer'],\n",
              " ['hate', 'swim', 'big', 'pool']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "n-dimensions = 200\n",
        "\n",
        "# We construct and train our own Word2Vec.\n",
        "model_word2vec = Word2Vec(sentences=, vector_size=dimensions, window=, min_count=1, epochs=10000, workers=4, negative=10)"
      ],
      "metadata": {
        "id": "uzdZzggWtPwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "f9b49739-31c2-495e-b110-af9ad35de431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (<ipython-input-6-2914dc6df27a>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-2914dc6df27a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    n-dimensions = 200\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All words captured by the model:\", model_word2vec.wv.key_to_index)\n",
        "\n",
        "word = 'love'\n",
        "print(\"The embedding of\", word, \"is\", model_word2vec.)\n",
        "\n",
        "# Get the embedding for each word captured by the model.\n",
        "words = model_word2vec.\n",
        "embeddings = np.array([model_word2vec.wv[word] for word in words])"
      ],
      "metadata": {
        "id": "nLQrf-95ns7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "id": "Dxc1rwHLturY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten words have ten embeddings. Each word has a n-dimensional embedding (i.e., vector_size)"
      ],
      "metadata": {
        "id": "hs0cagBTuM2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot a 3D PCA plot to see these embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "h1WUTbGboozX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "def plot_scatter_3d(model, embeddings):\n",
        "  dim_red = PCA(n_components=3, random_state=42)\n",
        "\n",
        "  embeddings_for_visualization = dim_red.fit_transform(embeddings)\n",
        "\n",
        "  # Convert the reduced embeddings and words into a DataFrame\n",
        "  df = pd.DataFrame(embeddings_for_visualization, columns=['x', 'y', 'z'])\n",
        "  df['word'] = [ word for word in model_word2vec.wv.index_to_key]\n",
        "\n",
        "  # Create a scatter plot using Plotly\n",
        "  fig = px.scatter_3d(df, x='x', y='y', z='z', text='word', title='Word Embeddings Visualization')\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "3AdAs95iotd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter_3d(model_word2vec, embeddings)"
      ],
      "metadata": {
        "id": "SEPzJZpZrJZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how this maps to a pre-trained embedding model (GloVe or Word2Vec)"
      ],
      "metadata": {
        "id": "mpJj97p1qFbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pretrained model\n",
        "# pretrained_model = api.load('word2vec-google-news-300')\n",
        "pretrained_model = api.load('glove-wiki-gigaword-200')\n",
        "# pretrained_model = api.load('glove-twitter-200')\n"
      ],
      "metadata": {
        "id": "qM_zysfCsQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking of the model does not recognize any of the words"
      ],
      "metadata": {
        "id": "ciDJ3pUeD1b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[word  for word in words if word not in pretrained_model]"
      ],
      "metadata": {
        "id": "rSYUZ_u_u1Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = pretrained_model.vector_size\n",
        "\n",
        "embeddings = np.array([\n",
        "     # if the word is not recognized, replace it with a vector of zeros\n",
        "    for word in words\n",
        "])\n",
        "\n",
        "plot_scatter_3d(pretrained_model, embeddings)"
      ],
      "metadata": {
        "id": "vURL4bjGqVB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Application: Using Neural Embeddings for Spam Detection\n",
        "\n",
        "Now that we were able to represent the words using the pre-trained embeddings, let's apply it to our spam detection problem."
      ],
      "metadata": {
        "id": "vubWYBXjbG8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/elhamod/BA820/main/Hands-on/04-text-mining/hamspam.csv\"\n",
        "df_sms = pd.read_csv(url, names = ['type', 'text'], index_col='type')\n",
        "\n",
        "X = df_sms['text']\n",
        "y = df_sms.index\n",
        "\n",
        "df_sms"
      ],
      "metadata": {
        "id": "BJYnG9pm7iVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, do some pre-processing."
      ],
      "metadata": {
        "id": "EjTOujij1Fhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = df_sms['text'][0]\n",
        "message"
      ],
      "metadata": {
        "id": "8B8Fd9eVtfw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a message:\", cleanup_text(message)) # cleanup_text(message), message\n",
        "\n",
        "print(\"Embedding of the entir message:\",pretrained_model.get_mean_vector(message))"
      ],
      "metadata": {
        "id": "T0ZNR-28KR6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = df_sms['text']\n",
        "tokenized_messages = [cleanup_text(message) for message in messages]"
      ],
      "metadata": {
        "id": "gQXHPxZPyTRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to calculate sentence embeddings, let's average the word embeddings."
      ],
      "metadata": {
        "id": "sqnyMhAZFrKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vector_size = pretrained_model.vector_size  # Get the embedding size\n",
        "\n",
        "vectorized_messages = [\n",
        "     # f no tokens are recognized, use a zero vector\n",
        "    for sentence in tokenized_messages\n",
        "]"
      ],
      "metadata": {
        "id": "L8kt2TBuLBKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the embeddings are constructed, we can split to train/test sets and use supervised learning."
      ],
      "metadata": {
        "id": "nomzk3XypXza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def assess_model(df, embeddings):\n",
        "  # train/test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(embeddings, df.index, test_size=0.2, random_state=42)\n",
        "\n",
        "  # train the model\n",
        "  classifier = LogisticRegression()\n",
        "  classifier.fit(X_train, y_train)\n",
        "\n",
        "  # Predict on the test data\n",
        "  y_pred = classifier.predict(X_test)\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  f1_score = sklearn.metrics.f1_score(y_test, y_pred, pos_label=\"spam\")\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  print(f\"f1_score: {f1_score}\")\n",
        "  print(sklearn.metrics.classification_report(y_test,y_pred))\n",
        "  display(pd.DataFrame(confusion_matrix(y_test, y_pred, normalize='true'), columns=classifier.classes_, index=classifier.classes_ ))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ENDlaGeNcQJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assess_model(df_sms, vectorized_messages)"
      ],
      "metadata": {
        "id": "SiqH5wEIbuz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Misc Functions"
      ],
      "metadata": {
        "id": "QFawDRNm2utb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find words that are most similar to a word"
      ],
      "metadata": {
        "id": "ze-HnzUB2ysR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'astrology'\n",
        "\n",
        "pretrained_model.similar_by_word(word) # , topn=5"
      ],
      "metadata": {
        "id": "MI-3auGh25Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find word analogies"
      ],
      "metadata": {
        "id": "-Ri_nu9Y3Fip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " pretrained_model.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "metadata": {
        "id": "B9iCFLHD3FGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find cosine similarity between two sentences"
      ],
      "metadata": {
        "id": "qfj7ks7Q3KP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " pretrained_model.n_similarity(word_tokenize('I like it'), word_tokenize('hate it'))"
      ],
      "metadata": {
        "id": "B2GglrG72yQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "- Would dimensionality reduction help improve the results?\n",
        "- Would you be able to use clustering to find different of messages? Do the clusters align with the ham/spam split?\n",
        "- Visualize the dataset using non-linear methods."
      ],
      "metadata": {
        "id": "vuhqikhwDUr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Using Deep Learning Embeddings"
      ],
      "metadata": {
        "id": "yinPOGewfRuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just saw how embeddings like Word2Vec can help us represent text as vectors to perform downstream tasks, such as classification.\n",
        "\n",
        "Let's try now more advanced deep learning models that produce more sophisticated embeddings.\n",
        "\n",
        "We will use `DistelBERT` through [`huggingface`](https://huggingface.co/). `huggingface` is a widely used platfrom for datasets and deep learning models, especially Transformers.\n",
        "\n"
      ],
      "metadata": {
        "id": "c22J2hG0f0Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "re-z5cNIjU0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "st_model = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\n",
        "\n",
        "embeddings = st_model.\n"
      ],
      "metadata": {
        "id": "zmmXrahJOrXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assess_model(df_sms, embeddings)"
      ],
      "metadata": {
        "id": "U6GGfS3upY3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using a Pretrained Model"
      ],
      "metadata": {
        "id": "Jr4jg88b7qh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of extracting embeddings and then training logistic regression, how about we use a pre-trained deep learning model (a Transformer)?\n",
        "\n",
        "Searching `huggingface` for a suitable model for ham/spam, one could find the following [Bert_Spam_ham](https://huggingface.co/saadkiet/Fine_Tuned_bert_Spam_ham) model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6iuFgMg7tZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n"
      ],
      "metadata": {
        "id": "ZqczAsnu8EN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try on one sentence."
      ],
      "metadata": {
        "id": "t0TynTDc98P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(  )"
      ],
      "metadata": {
        "id": "aR4oOhGZ9dBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice here that while the model internally computed the embeddings, it give us the final classification, along with the score indicating its certaining. So, we do not need to train a separate classifier."
      ],
      "metadata": {
        "id": "xv1vlUhW-KJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assess_model_bert(df, model):\n",
        "  # train/test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df.index, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Predict on the test data\n",
        "  y_pred = model(X_test.to_list())\n",
        "  y_pred = [int(x[\"label\"][-1]) for x in y_pred]\n",
        "  y_pred = [\"ham\" if x == 0 else \"spam\" for x in y_pred]\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy2 = accuracy_score(y_test, y_pred)\n",
        "  f1_score = sklearn.metrics.f1_score(y_test, y_pred, pos_label=\"spam\")\n",
        "  print(f\"Accuracy: {accuracy2}\")\n",
        "  print(f\"f1_score: {f1_score}\")\n",
        "  print(sklearn.metrics.classification_report(y_test,y_pred))\n",
        "  display(pd.DataFrame(confusion_matrix(y_test, y_pred, normalize='true'), columns=[\"ham\", \"spam\"], index=[\"ham\", \"spam\"]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9e4PUAAf9iJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assess_model_bert(df_sms, pipe)"
      ],
      "metadata": {
        "id": "UPQ5fw7Y-0D-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}